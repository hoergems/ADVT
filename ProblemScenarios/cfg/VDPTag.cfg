# General-purpose settings.
verbose = false
logPath = log
overwriteExistingLogFiles = true
logFilePostfix = 
saveParticles = false

[plugins]
heuristicPlugin = libvdpTagHeuristicPlugin.so

planningRewardPlugin = libvdpTagRewardPlugin.so
executionRewardPlugin = libvdpTagRewardPlugin.so

planningTerminalPlugin = libvdpTagTerminalPlugin.so
executionTerminalPlugin = libvdpTagTerminalPlugin.so

planningTransitionPlugin = libvdpTagTransitionPlugin.so
executionTransitionPlugin = libvdpTagTransitionPlugin.so

planningObservationPlugin = libvdpTagObservationPlugin.so
executionObservationPlugin = libvdpTagObservationPlugin.so

executionInitialBeliefPlugin = libvdpTagInitialBeliefPlugin.so
planningInitialBeliefPlugin = libvdpTagInitialBeliefPlugin.so

[problem]
robotName = Agent

# Number of simulation runs
nRuns = 1

# Maximum number of steps to reach the goal
nSteps = 50

enableGazeboStateLogging = false

# The discount factor of the reward model
discountFactor = 0.95

# The maximum time to spend on each step, in milliseconds (0 => no time limit)
stepTimeout = 1000

# The planning environment SDF
planningEnvironmentPath = VDPTag.sdf

# The execution environment SDF
executionEnvironmentPath = VDPTag.sdf

##################################################################################################
##################################################################################################
#### State, action and observation description
##################################################################################################
##################################################################################################
[state]
additionalDimensions = 4
additionalDimensionLimits = [[-4, 4], [-4, 4]]

[action]
additionalDimensions = 2
additionalDimensionLimits = [[0, 6.283185307], [0, 1]]

[observation]
additionalDimensions = 8
additionalDimensionLimits = [[0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1]]

[ADVT]
numEpisodes = 0
maximumDepth = 10
minParticleCount = 50000

explorationFactor = 156.6
explorationFactorDiameter = 5.0
splitExplorationFactor = 5.0
maxObservationDistance = 26.6

numDiameterSamples = 10
partitioningMode = VDP
distanceMeasure = VDP

resetTree = false
rejectionSampling = false
bellmanBackup = true

[simulation]
interactive = false
particlePlotLimit = 0
