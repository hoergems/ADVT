# General-purpose settings.
verbose = false
logPath = log
overwriteExistingLogFiles = true
logFilePostfix =

# If this is set to 'true', the belief particles are written to the log file.
# WARNING: log files can become huge!
saveParticles = false

[plugins]
heuristicPlugin = libpushboxHeuristicPlugin.so

planningRewardPlugin = libpushboxRewardPlugin.so
executionRewardPlugin = libpushboxRewardPlugin.so

planningTerminalPlugin = libpushboxTerminalPlugin.so
executionTerminalPlugin = libpushboxTerminalPlugin.so

planningTransitionPlugin = libpushboxTransitionPlugin.so
executionTransitionPlugin = libpushboxTransitionPlugin.so

planningObservationPlugin = libpushboxObservationPlugin.so
executionObservationPlugin = libpushboxObservationPlugin.so

executionInitialBeliefPlugin = libpushboxInitialBeliefPlugin.so
planningInitialBeliefPlugin = libpushboxInitialBeliefPlugin.so

[initialBeliefOptions]
initialStateVec = [5.5, 9.5, 0.0, 5.5, 5.5, 0.0]
initialBoxPositionUncertainty = 2.0

[transitionPluginOptions]
actionUncertainty = 0.0
moveUncertainty = 0.0
boxSpeedUncertainty = 0.1
boxPositionMoveUncertainty = 0.1

[observationPluginOptions]
observationUncertainty = 10
observationBuckets = 12

usePositionObservation = false
positionObservationUncertainty = 0.2

[rewardPluginOptions]
moveCost = 10.0
goalReward = 1000.0
collisionPenalty = 1000.0

[map]
mapPath = /home/uqmhoerg/POMDPProblems/Pushbox/cfg/map_free_paper.txt
sizeX = 10
sizeY = 10

goalPosition = [8.5, 9.5, 0.0]
goalRadius = 0.5

[problem]
# Number of simulation runs
nRuns = 1

# Maximum number of steps to reach the goal
nSteps = 50

# The discount factor of the problem
discountFactor = 0.95

# The maximum time to spend on each step, in milliseconds (0 => no time limit)
stepTimeout = 1000

# The planning environment SDF
planningEnvironmentPath = Pushbox.sdf

# The execution environment SDF
executionEnvironmentPath = Pushbox.sdf

robotName = PushboxRobot

##################################################################################################
##################################################################################################
#### State, action and observation description
##################################################################################################
##################################################################################################
[state]
additionalDimensions = 6
additionalDimensionLimits = [[-10.0, 10.0], [-10.0, 10.0], [-10.0, 10.0], [-10.0, 10.0], [-10.0, 10.0], [-10.0, 10.0]]

[action]
additionalDimensions = 3
additionalDimensionLimits = [[-1.0, 1.0], [-1.0, 1.0], [-1.0, 1.0]]

[observation]
additionalDimensions = 3
additionalDimensionLimits = [[0, 200], [0, 200], [0, 200]]

[ADVT]
explorationFactor = 62.5
explorationFactorDiameter = 5.0
splitExplorationFactor = 1.0
minimumSplittingDiam = 1e-4

bellmanBackup = true
numDiameterSamples = 20
partitioningMode = VORONOI
resetTree = false
minParticleCount = 50000
numEpisodes = 0
maximumDepth = 2

rejectionSampling = true
maxObservationDistance = 0.1

[simulation]
interactive = false
particlePlotLimit = 0
