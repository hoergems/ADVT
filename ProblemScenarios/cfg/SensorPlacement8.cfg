# General-purpose settings.
verbose = false
logPath = log
overwriteExistingLogFiles = true
logFilePostfix =

# If this is set to 'true', the belief particles are written to the log file.
# WARNING: log files can become huge!
saveParticles = false

[plugins]
heuristicPlugin = libsensorPlacementHeuristicPlugin.so

planningRewardPlugin = libsensorPlacementRewardPlugin.so
executionRewardPlugin = libsensorPlacementRewardPlugin.so

planningTerminalPlugin = libsensorPlacementTerminalPlugin.so
executionTerminalPlugin = libsensorPlacementTerminalPlugin.so

planningTransitionPlugin = libsensorPlacementTransitionPlugin.so
executionTransitionPlugin = libsensorPlacementTransitionPlugin.so

planningObservationPlugin = libsensorPlacementObservationPlugin.so
executionObservationPlugin = libsensorPlacementObservationPlugin.so

executionInitialBeliefPlugin = libsensorPlacementInitialBeliefPlugin.so
planningInitialBeliefPlugin = libsensorPlacementInitialBeliefPlugin.so

[initialBeliefOptions]
initialStateVec = [0.0, -1.57, 1.57, 0.0, 0.0, 0.0, 0.0, 0.0]
lowerUpperBound = [0.075, 0.075, 0.075, 0.075, 0.075, 0.075, 0.075, 0.075]

[transitionPluginOptions]
endEffectorLink = 8DOFManipulator::endEffector
lowerTransitionErrorBound = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
upperTransitionErrorBound = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

[observationPluginOptions]
correctObservationProbability = 1.0

[rewardPluginOptions]
stepPenalty = 1.0
illegalMovePenalty = 500.0
exitReward = 1000

[problem]
# Number of simulation runs
nRuns = 1

# Maximum number of steps to reach the goal
nSteps = 50

# The discount factor of the problem
discountFactor = 0.95

# The maximum time to spend on each step, in milliseconds (0 => no time limit)
stepTimeout = 1000

# The planning environment SDF
planningEnvironmentPath = 8DOFSensorPlacementEnvironment.sdf

# The execution environment SDF
executionEnvironmentPath = 8DOFSensorPlacementEnvironment.sdf

robotName = 8DOFManipulator

##################################################################################################
##################################################################################################
#### State, action and observation description
##################################################################################################
##################################################################################################
[state]
jointPositions = [8DOFManipulator::joint1, 8DOFManipulator::joint2, 8DOFManipulator::joint3, 8DOFManipulator::joint4, 8DOFManipulator::joint5, 8DOFManipulator::joint6, 8DOFManipulator::joint7, 8DOFManipulator::joint8]

[action]
additionalDimensions = 8
additionalDimensionLimits = [[-0.5, 0.5], [-0.5, 0.5], [-0.5, 0.5], [-0.5, 0.5], [-0.5, 0.5], [-0.5, 0.5], [-0.5, 0.5], [-0.5, 0.5]]

[observation]
additionalDimensions = 1
additionalDimensionLimits = [[0.0, 1.0]]

[ADVT]
explorationFactor = 56.6
explorationFactorDiameter = 70.0
splitExplorationFactor = 20.0

partitioningMode = VORONOI
numDiameterSamples = 6

bellmanBackup = true
resetTree = false
maxObservationDistance = 0.1
minParticleCount = 30000
numEpisodes = 0
maximumDepth = 3
rejectionSampling = false

[simulation]
interactive = false
particlePlotLimit = 0
