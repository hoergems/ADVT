# General-purpose settings.
verbose = false
logPath = log
overwriteExistingLogFiles = true
logFilePostfix =

# If this is set to 'true', the belief particles are written to the log file.
# WARNING: log files can become huge!
saveParticles = false

[plugins]
heuristicPlugin = libpushboxHeuristicPlugin.so

planningRewardPlugin = libpushboxRewardPlugin.so
executionRewardPlugin = libpushboxRewardPlugin.so

planningTerminalPlugin = libpushboxTerminalPlugin.so
executionTerminalPlugin = libpushboxTerminalPlugin.so

planningTransitionPlugin = libpushboxTransitionPlugin.so
executionTransitionPlugin = libpushboxTransitionPlugin.so

planningObservationPlugin = libpushboxObservationPlugin.so
executionObservationPlugin = libpushboxObservationPlugin.so

executionInitialBeliefPlugin = libpushboxInitialBeliefPlugin.so
planningInitialBeliefPlugin = libpushboxInitialBeliefPlugin.so

[initialBeliefOptions]
# The initial XY-coordinates robot and the box
initialStateVec = [5.5, 9.5, 5.5, 5.5]
initialBoxPositionUncertainty = 2.0

[transitionPluginOptions]
actionUncertainty = 0.0
boxSpeedUncertainty = 0.1
boxPositionMoveUncertainty = 0.1
moveUncertainty = 0.0

[observationPluginOptions]
observationUncertainty = 10
observationBuckets = 12
usePositionObservation = false
positionObservationUncertainty = 0.2

[rewardPluginOptions]
moveCost = 10.0
goalReward = 1000.0
collisionPenalty = 1000.0

[map]
sizeX = 10
sizeY = 10

goalPosition = [8.5, 9.5]
goalRadius = 0.5

[problem]
# Number of simulation runs
nRuns = 1

# Maximum number of steps to reach the goal
nSteps = 50

# The discount factor of the problem
discountFactor = 0.95

# The maximum time to spend on each step, in milliseconds (0 => no time limit)
stepTimeout = 1000

# The planning environment SDF
planningEnvironmentPath = Pushbox.sdf

# The execution environment SDF
executionEnvironmentPath = Pushbox.sdf

robotName = PushboxRobot

##################################################################################################
##################################################################################################
#### State, action and observation description
##################################################################################################
##################################################################################################
[state]
additionalDimensions = 4
additionalDimensionLimits = [[-10.0, 10.0], [-10.0, 10.0], [-10.0, 10.0], [-10.0, 10.0]]

[action]
# Here we construct a 1-dimensional action space with range 1-13. ABT will discretize the action space into 13 discrete actions
additionalDimensions = 2
additionalDimensionLimits = [[-1.0, 1.0], [-1.0, 1.0]]

[observation]
additionalDimensions = 2
additionalDimensionLimits = [[0, 200], [0, 200]]

[ADVT]
explorationFactor = 27.5
explorationFactorDiameter = 5.0
splitExplorationFactor = 3.25
minimumSplittingDiam = 1e-4
bellmanBackup = true

numDiameterSamples = 20
partitioningMode = VORONOI

resetTree = false
minParticleCount = 30000
numEpisodes = 0
maximumDepth = 2
rejectionSampling = true
maxObservationDistance = 0.1

[simulation]
interactive = false
particlePlotLimit = 0
